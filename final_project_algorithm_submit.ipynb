{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project_algorithm_submit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BE 521 Final Project\n",
        "## Group: The Chicken Finger Feature Finders\n",
        "## Group Members: Carlos Aguila, Anthony LoPrete, Frederick Xu\n",
        "\n",
        "Instructions:\n",
        "1. Download \"final_submission_files.zip\" (~370 mb) from the following link: https://drive.google.com/file/d/1VmzfwPeyqEjlwJ6e_BF0SyCEyicAGLMJ/view?usp=sharing\n",
        "2. Upload \"truetest_data.mat\" and \"final_submission_files.zip\" to the content directory.\n",
        "3. Run the script, the notebook should do the rest.\n",
        "4. The code outputs \"predictions.mat\", which contains a variable `predicted_dg`, which is our predictions on the hidden test set.\n",
        "\n",
        "Some notes on \"final_submission_files.zip\":\n",
        "- There are 3 `.model` files corresponding to a pre-trained algorithm for each subject\n",
        "- There are 3 `.npy` files corresponding to the training features for each subject\n",
        "- The 3 `.npy` files are *only for normalizing the hidden test set*. Our code was originally built to normalize all feature sets together in one function so these arrays are necessary for that code to work."
      ],
      "metadata": {
        "id": "lPpHtmR6WvDl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jFrJOtT0nC9v"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import scipy\n",
        "from scipy.stats import pearsonr\n",
        "from scipy import signal as sig\n",
        "from scipy.io import loadmat, savemat\n",
        "from scipy.interpolate import CubicSpline\n",
        "from scipy.signal import ellip, lfilter, filtfilt, find_peaks, butter, sosfiltfilt,sosfilt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"final_submission_files.zip\" -d \".\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5rki9bi5-zP",
        "outputId": "b0f47f95-d9cf-4517-998b-16b5b81771b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  final_submission_files.zip\n",
            "  inflating: ./all_feats_s1_train_fx.npy  \n",
            "  inflating: ./all_feats_s2_train_fx.npy  \n",
            "  inflating: ./all_feats_s3_train_fx.npy  \n",
            "  inflating: ./subject1_rfr_1000_run2.model  \n",
            "  inflating: ./subject2_rfr_1000_run2.model  \n",
            "  inflating: ./subject3_rfr_1000_run2.model  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = loadmat('truetest_data.mat')\n",
        "\n",
        "#leaderboard ecog signal per patient\n",
        "test_data_s1 = test_data[list(test_data.keys())[-1]][0][0]\n",
        "test_data_s1 = np.delete(test_data_s1, 54, axis=1)\n",
        "\n",
        "test_data_s2 = test_data[list(test_data.keys())[-1]][1][0]\n",
        "test_data_s2 = np.delete(test_data_s2, [20, 37], axis=1)\n",
        "\n",
        "test_data_s3 = test_data[list(test_data.keys())[-1]][2][0]"
      ],
      "metadata": {
        "id": "4pyyDwgrwfZM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Signal pre-processing\n",
        "\n",
        "We used an 8th-order Butterworth bandpass filter with a lower cutoff of 0.15 Hz and upper cutoff of 200 Hz. "
      ],
      "metadata": {
        "id": "lPjNtKJo1_N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data(raw_eeg, fs=1000):\n",
        "  \"\"\"\n",
        "  Write a filter function to clean underlying data.\n",
        "  Filter type and parameters are up to you. Points will be awarded for reasonable filter type, parameters and application.\n",
        "  Please note there are many acceptable answers, but make sure you aren't throwing out crucial data or adversly\n",
        "  distorting the underlying data!\n",
        "\n",
        "  Input: \n",
        "    raw_eeg (samples x channels): the raw signal\n",
        "    fs: the sampling rate (1000 for this dataset)\n",
        "  Output: \n",
        "    clean_data (samples x channels): the filtered signal\n",
        "  \"\"\"\n",
        "  raw_eeg_t = raw_eeg.transpose()\n",
        "  filtered = []\n",
        "\n",
        "  nyq = fs/2\n",
        "\n",
        "  sos = butter(8, [0.15,200],btype='bandpass',output='sos',fs=fs)\n",
        "\n",
        "  for ch_data in raw_eeg_t:\n",
        "    filtered_ch = sosfiltfilt(sos,ch_data)\n",
        "    filtered.append(filtered_ch)\n",
        "  \n",
        "  filtered = np.array(filtered)\n",
        "\n",
        "  return filtered.transpose()"
      ],
      "metadata": {
        "id": "xkCFpSFz1urg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filt_test_ecog_s1 = filter_data(test_data_s1)\n",
        "filt_test_ecog_s2 = filter_data(test_data_s2)\n",
        "filt_test_ecog_s3 = filter_data(test_data_s3)"
      ],
      "metadata": {
        "id": "v8ygX-hW108g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "\n",
        "We extract several different features:\n",
        "- Line Length\n",
        "- Energy\n",
        "- Biorhythm bandpower (Delta, Theta, Alpha, Beta, Gamma)\n",
        "\n",
        "Please note: this step does take some time to run (~10 mins) to extract all of the features."
      ],
      "metadata": {
        "id": "4mck45VF2XTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#number of windows in signal given winLen and winDisp\n",
        "def NumWins(x, fs, winLen, winDisp):\n",
        "  return (len(x) - winLen*fs + winDisp*fs) // (winDisp * fs)\n",
        "\n",
        "#line length\n",
        "def LL(x):\n",
        "  return np.sum(np.absolute(np.ediff1d(x)))\n",
        "\n",
        "#energy\n",
        "def E(x):\n",
        "  return np.sum(x**2)\n",
        "\n",
        "# Bandpower, specifying the following:\n",
        "# Delta: fmin = 0.5, fmax = 4\n",
        "# Theta: fmin = 4, fmax = 7\n",
        "# Alpha: fmin = 8, fmax = 12\n",
        "# Beta: fmin = 12.5, fmax = 30\n",
        "# Gamma: fmin = 25, fmax = 140\n",
        "def bandpower(x, fs, fmin, fmax):\n",
        "    f, Pxx = scipy.signal.periodogram(x, fs=fs)\n",
        "    ind_min = np.argmax(f > fmin) - 1\n",
        "    ind_max = np.argmax(f > fmax) - 1\n",
        "    return np.trapz(Pxx[ind_min: ind_max], f[ind_min: ind_max])\n",
        "\n",
        "#gets features, load features you want calculated from here\n",
        "def get_features(filtered_window, fs=1000):\n",
        "  \"\"\"\n",
        "    Write a function that calculates features for a given filtered window. \n",
        "    Feel free to use features you have seen before in this class, features that\n",
        "    have been used in the literature, or design your own!\n",
        "\n",
        "    Input: \n",
        "      filtered_window (window_samples x channels): the window of the filtered ecog signal \n",
        "      fs: sampling rate\n",
        "    Output:\n",
        "      features (channels x num_features): the features calculated on each channel for the window\n",
        "  \"\"\"\n",
        "\n",
        "  filtered_window_t = filtered_window.transpose()\n",
        "\n",
        "  features = []\n",
        "\n",
        "  for ch in filtered_window_t:\n",
        "    #features.append(np.array([LL(ch), E(ch), SP_5(ch), SP_20(ch), SP_75(ch), SP_125(ch), SP_160(ch)]))\n",
        "    \n",
        "    features.append(np.array([LL(ch), # Line-Length\n",
        "                              E(ch), # Energy\n",
        "                              bandpower(ch, fs, 0.5, 4), # Delta\n",
        "                              bandpower(ch, fs, 4, 7), # Theta\n",
        "                              bandpower(ch, fs, 8, 12), # Alpha\n",
        "                              bandpower(ch, fs, 12.5, 30), # Beta\n",
        "                              bandpower(ch, fs, 25, 140) # Gamma\n",
        "                             ])) \n",
        "    \n",
        "  features = np.array(features)\n",
        "\n",
        "  return features\n",
        "\n",
        "#get_windowed_feats - filters raw ecog signal and finds features\n",
        "def get_windowed_feats(raw_ecog, fs, window_length, window_overlap):\n",
        "  \"\"\"\n",
        "    Write a function which processes data through the steps of filtering and\n",
        "    feature calculation and returns features. Points will be awarded for completing\n",
        "    each step appropriately (note that if one of the functions you call within this script\n",
        "    returns a bad output, you won't be double penalized). Note that you will need\n",
        "    to run the filter_data and get_features functions within this function. \n",
        "\n",
        "    Inputs:\n",
        "      raw_eeg (samples x channels): the raw signal\n",
        "      fs: the sampling rate (1000 for this dataset)\n",
        "      window_length: the window's length\n",
        "      window_overlap: the window's overlap\n",
        "    Output: \n",
        "      all_feats (num_windows x (channels x features)): the features for each channel for each time window\n",
        "        note that this is a 2D array. \n",
        "  \"\"\"\n",
        "\n",
        "  cleaned_ecog = filter_data(raw_ecog)\n",
        "  num_wins = NumWins(cleaned_ecog.transpose()[0], fs, window_length, window_overlap)\n",
        "  all_feats_3d = []\n",
        "  for winStart in np.arange(0, int(num_wins), 1):\n",
        "    clip = cleaned_ecog[int(winStart*window_overlap*fs):int(winStart*window_overlap*fs + (window_length *fs))]\n",
        "    all_feats_3d.append(get_features(clip))\n",
        "\n",
        "  num_channels = len(all_feats_3d[0])\n",
        "  num_features = len(all_feats_3d[0][0])\n",
        "\n",
        "  all_feats = np.zeros([len(all_feats_3d),num_features*num_channels])\n",
        "\n",
        "  for k in range(int(len(all_feats_3d))):\n",
        "    q = flatten_list = [j for sub in all_feats_3d[k] for j in sub]\n",
        "    all_feats[k,:] = q\n",
        "\n",
        "  return np.array(all_feats)#, np.array(all_feats_3d)"
      ],
      "metadata": {
        "id": "-84PXSEh19rH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_feats_test_s1 = get_windowed_feats(filt_test_ecog_s1, 1000, 0.100, 0.050); #output of get_windowed_feats\n",
        "all_feats_test_s2 = get_windowed_feats(filt_test_ecog_s2, 1000, 0.100, 0.050); #output of get_windowed_feats\n",
        "all_feats_test_s3 = get_windowed_feats(filt_test_ecog_s3, 1000, 0.100, 0.050); #output of get_windowed_feats"
      ],
      "metadata": {
        "id": "mOub1KG92WNo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_feats_test_s1.shape)\n",
        "print(all_feats_test_s2.shape)\n",
        "print(all_feats_test_s3.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIA9mnXfPLqS",
        "outputId": "a80fc8d3-5162-430e-c1ed-6529620edee0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2949, 427)\n",
            "(2949, 322)\n",
            "(2949, 448)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing the Features\n",
        "\n",
        "We normalize the testing data based on the mean and standard deviation of the training data. For this, we have included the training data as part of the algorithm files and it is loaded accordingly after unzipping.\n",
        "\n",
        "Please ignore the warnings, it is because some of the features ended up with all 0's resulting in a standard deviation of 0."
      ],
      "metadata": {
        "id": "fF36oXyT7hxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(all_feats_train, all_feats_test, num_features):\n",
        "  # Input should be (num_windows x (channels x features))\n",
        "  # Num features is the number of unique features that were extracted\n",
        "  all_feats_train_norm = np.copy(all_feats_train)\n",
        "  all_feats_test_norm = np.copy(all_feats_test)\n",
        "  \n",
        "  feats_avg = []\n",
        "  feats_std = []\n",
        "  \n",
        "  for n in range(num_features):\n",
        "    feats_idx_train = np.arange(n,len(all_feats_train.transpose()),num_features)\n",
        "    \n",
        "    feat_data_train = all_feats_train[:][:,feats_idx_train]\n",
        "    \n",
        "    feat_means_train = np.mean(feat_data_train,axis=0)\n",
        "    feat_stds_train = np.std(feat_data_train,axis=0)\n",
        "    \n",
        "    all_feats_train_norm[:][:,feats_idx_train] = (feat_data_train - feat_means_train)/feat_stds_train\n",
        "    \n",
        "    # Note that we must use the same mean and std. dev from the TRAINING set\n",
        "    #     because regression models are sensitive to value domain as they are\n",
        "    #     scale-variant.\n",
        "    feats_idx_test = np.arange(n,len(all_feats_test.transpose()),num_features)\n",
        "    feat_data_test = all_feats_test[:][:,feats_idx_test]\n",
        "    all_feats_test_norm[:][:,feats_idx_test] = (feat_data_test - feat_means_train)/feat_stds_train\n",
        "    \n",
        "    # Sanity checking plot, comment out if you don't want plots\n",
        "    # if n == 0:\n",
        "    #     plt.figure()\n",
        "    #     plt.plot(feat_data_train.transpose()[0])\n",
        "    #     plt.figure()\n",
        "    #     plt.plot(all_feats_train_norm[:][:,feats_idx_train].transpose()[0])\n",
        "    \n",
        "    #     plt.figure()\n",
        "    #     plt.plot(feat_data_test.transpose()[0])\n",
        "    #     plt.figure()\n",
        "    #     plt.plot(all_feats_test_norm[:][:,feats_idx_test].transpose()[0])\n",
        "\n",
        "  return all_feats_train_norm, all_feats_test_norm"
      ],
      "metadata": {
        "id": "NC01gYHy7NZ-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_s1 = open(\"./all_feats_s1_train_fx.npy\", \"rb\")\n",
        "feats_s1_train = np.load(file_s1)\n",
        "file_s1.close()\n",
        "\n",
        "file_s2 = open(\"./all_feats_s2_train_fx.npy\", \"rb\")\n",
        "feats_s2_train = np.load(file_s2)\n",
        "file_s2.close()\n",
        "\n",
        "file_s3 = open(\"./all_feats_s3_train_fx.npy\", \"rb\")\n",
        "feats_s3_train = np.load(file_s3)\n",
        "file_s3.close()"
      ],
      "metadata": {
        "id": "qzkMOgUG7eu_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, all_feats_test_norm_s1 = normalize_features(feats_s1_train, all_feats_test_s1, 7)\n",
        "_, all_feats_test_norm_s2 = normalize_features(feats_s2_train, all_feats_test_s2, 7)\n",
        "_, all_feats_test_norm_s3 = normalize_features(feats_s3_train, all_feats_test_s3, 7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DXxIpRQKN8H",
        "outputId": "c3440eaf-7346-4b36-f305-de8c14dd8427"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping some of the features\n",
        "\n",
        "We found that the lower-frequency biorhythm bandpowers did not provide much information, with almost all of the values being 0, so we dropped them after the extraction process."
      ],
      "metadata": {
        "id": "EWYQZ4SoQFn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problematic features, drop them for now. \n",
        "def clean_features(feats):\n",
        "    bad_feat_inds = np.concatenate((np.arange(2,len(feats.transpose()),7),\n",
        "                                   np.arange(3,len(feats.transpose()),7),\n",
        "                                   np.arange(4,len(feats.transpose()),7),\n",
        "                                  ))\n",
        "    feats_cleaned = np.delete(feats, bad_feat_inds, axis=1)\n",
        "    \n",
        "    return feats_cleaned\n",
        "\n",
        "all_feats_test_norm_s1 = clean_features(all_feats_test_norm_s1)\n",
        "all_feats_test_norm_s2 = clean_features(all_feats_test_norm_s2)\n",
        "all_feats_test_norm_s3 = clean_features(all_feats_test_norm_s3)"
      ],
      "metadata": {
        "id": "BKdVZljUQBPn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction using trained models\n",
        "\n",
        "There are 3 individual models for each subject that have been trained."
      ],
      "metadata": {
        "id": "SdmRPRrfVEQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_rfr_s1 = pickle.load(open('subject1_rfr_1000_run2.model', 'rb'))\n",
        "pred_s1 = reg_rfr_s1.predict(all_feats_test_norm_s1)\n",
        "\n",
        "reg_rfr_s2 = pickle.load(open('subject2_rfr_1000_run2.model', 'rb'))\n",
        "pred_s2 = reg_rfr_s2.predict(all_feats_test_norm_s2)\n",
        "\n",
        "reg_rfr_s3 = pickle.load(open('subject3_rfr_1000_run2.model', 'rb'))\n",
        "pred_s3 = reg_rfr_s3.predict(all_feats_test_norm_s3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBgz8EoGVDri",
        "outputId": "33f4f7de-e48c-4490-bc0e-45a208abdd29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator RandomForestRegressor from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-processing the predictions\n",
        "\n",
        "We first upsampled the predictions using a cubic spline, and then gaussian filtered the outputs to remove noise from the predictions."
      ],
      "metadata": {
        "id": "87bfNZR6Vpbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spline_preds(preds, time_length):\n",
        "  # N samples\n",
        "  preds_sample_orig = np.arange(len(preds))\n",
        "  \n",
        "  # T time points\n",
        "  preds_sample_target = np.linspace(0,len(preds),time_length)\n",
        "  preds = preds.transpose()\n",
        "\n",
        "  preds_interp = []\n",
        "  \n",
        "  for finger_preds in preds:\n",
        "      f = CubicSpline(preds_sample_orig, finger_preds, bc_type=\"natural\")\n",
        "      new_preds = f(preds_sample_target)\n",
        "      preds_interp.append(new_preds)\n",
        "  \n",
        "  preds_interp = np.array(preds_interp).transpose()\n",
        "  \n",
        "  return preds_interp\n",
        "\n",
        "\n",
        "def convolve_gaussian(preds):\n",
        "  preds_t = preds.transpose()\n",
        "  preds_t_convolve = []\n",
        "  \n",
        "  fs = 1000\n",
        "  gaussian_filter = np.exp(-1*(np.arange(int(-1*1000),int(1*1000)))**2/(0.75*1000)**2)\n",
        "  gaussian_filter_scaled = 1/np.sum(gaussian_filter) * gaussian_filter\n",
        "  \n",
        "  for row in preds_t:\n",
        "      preds_t_convolve.append(np.convolve(gaussian_filter_scaled, row, \"same\"))\n",
        "  \n",
        "  return np.array(preds_t_convolve).transpose()\n",
        "    \n",
        "\n",
        "pred1_test_postprocess = convolve_gaussian(spline_preds(pred_s1, len(test_data_s1)))\n",
        "pred2_test_postprocess = convolve_gaussian(spline_preds(pred_s2, len(test_data_s2)))\n",
        "pred3_test_postprocess = convolve_gaussian(spline_preds(pred_s3, len(test_data_s3)))\n",
        "\n",
        "print(np.shape(pred1_test_postprocess))\n",
        "print(np.shape(pred2_test_postprocess))\n",
        "print(np.shape(pred3_test_postprocess))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrXtad4RVo2z",
        "outputId": "24ea64c1-e46b-4c44-c456-d4fe322dab65"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(147500, 5)\n",
            "(147500, 5)\n",
            "(147500, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_array = np.zeros((3,1), dtype=object)\n",
        "predictions_array[0,0] = pred1_test_postprocess\n",
        "predictions_array[1,0] = pred2_test_postprocess\n",
        "predictions_array[2,0] = pred3_test_postprocess\n",
        "\n",
        "savemat('predictions.mat', {'predicted_dg':predictions_array})\n",
        "\n",
        "print(\"Predictions saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSI1-fKGVh4T",
        "outputId": "003d4c0e-4c2e-4108-9ffe-075ff3d2c6f3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved.\n"
          ]
        }
      ]
    }
  ]
}